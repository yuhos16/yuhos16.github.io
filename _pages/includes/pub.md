# ğŸ“š Publications
*# Equal Contribution*
## ğŸ©º AI-Driven Intelligence System for Dermtoalogy 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2026 Â· Under Review</div><img src='images/skingpt_r1.jpg' alt="SkinGPT-R1" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SkinGPT-R1: A Minimal-Parameter Dermatology Reasoning Model via Adapter-Only Dual Distillation on Visionâ€“R1](https://arxiv.org/abs/2511.15242) \\
**Yuhao Shen#**, Jiahe Qian#, Zhangtianyi Chen, Yuanhao He, Juexiao Zhou

<!-- [**Project**](https://github.com/placeholder/SkinGPT-R1) \| [**Code**](https://github.com/placeholder/SkinGPT-R1)  <strong><span class='show_paper_citations' data='PLACEHOLDER_SKINGPT_R1'></span></strong> -->

**SkinGPT-R1** æ˜¯ä¸€ä¸ªä¸“æ³¨çš®è‚¤ç§‘çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ˜¾å¼ã€é€æ­¥ã€å¯éªŒè¯åœ°æ‰§è¡Œè¯Šæ–­ Chain-of-Thought æ¨ç†ã€‚æˆ‘ä»¬æ„å»º **DermCoT**ï¼ˆæ ‡å‡†åŒ–çš®è‚¤ç§‘ CoT è¯­æ–™ï¼Œå« 10,000 æ¡ DermEval è¿‡æ»¤è®­ç»ƒæ ·æœ¬ä¸ 3,000 æ¡çš®è‚¤ç§‘ä¸“å®¶è¯„åˆ†æ ·æœ¬ï¼‰ï¼Œå¹¶æå‡º **DermEval**ï¼ˆä¸åŒ»ç”Ÿä¸€è‡´çš„å…­ç»´è¯„ä¼°å™¨ï¼‰åŠå¯¹åº”åŸºå‡† **DermBench**ã€‚åœ¨ DermBench ä¸Šï¼Œè·¨ 14 ä¸ªé€šç”¨/æ¨ç†/åŒ»å­¦ VLMï¼Œ**SkinGPT-R1** åœ¨å…­ä¸ªç»´åº¦ä¸Šå–å¾—å¹³å‡ **4.031/5** çš„å¾—åˆ†ï¼Œæ’åç¬¬ 1ï¼Œç›¸æ¯” Vision-R1 æå‡çº¦ **41%**ã€‚åœ¨ä¸‰é¡¹çš®è‚¤ç§‘åˆ†ç±»åŸºå‡†ä¸Šï¼Œ**SkinGPT-R1** ç›¸æ¯” Vision-R1 ä¹Ÿè·å¾—ç¨³å®šçš„å‡†ç¡®ç‡æå‡å¹¶ä¿æŒç«äº‰åŠ›ã€‚æ¶ˆèå®éªŒæ˜¾ç¤ºï¼šåŸºäº **DermCoT** çš„ CoT ç›‘ç£æ˜¾è‘—ä¼˜äºåŸºçº¿ï¼›åŒæ—¶åŠ å…¥é¢å‘çš®è‚¤ç—…å­¦çš„è§†è§‰è’¸é¦å¯åœ¨å™è¿°è´¨é‡ä¸è¯†åˆ«ä¸Šå¸¦æ¥ä¸€è‡´å¢ç›Šã€‚
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Scientific Data Â· Under Review</div><img src='images/skincare.png' alt="SkinCaRe Dataset" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SkinCaRe: A Multimodal Dermatology Dataset Annotated with Medical Caption and Chain-of-Thought Reasoning](https://arxiv.org/abs/2405.18004) \\
**Yuhao Shen#**, Liyuan Sun#, Yan Xu#, Wenbin Liu#, Shuping Zhang#, Shawn Afvari, Zhongyi Han, Jiaoyan Song, Yongzhi Ji, Tao Lu, Xiaonan He, Xin Gao, Juexiao Zhou

[**Dataset**](https://huggingface.co/datasets/yuhos16/SkinCaRe)
 <!-- \| [**Project**](https://github.com/placeholder/SkinCaRe)  <strong><span class='show_paper_citations' data='PLACEHOLDER_SKINCARE'></span></strong> -->

- Release **SkinCaRe**, unifying SkinCAP (medical captions) and SkinCoT (clinician-verified chains-of-thought) for transparent dermatologic reasoning.
- Enables training and evaluation of VLMs that both **describe** findings and **explain** diagnostic steps.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2026 Â· Under Review</div><img src='images/dermeval.jpg' alt="DermBench & DermEval" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Towards Trustworthy Dermatology MLLMs: A Benchmark and Multimodal Evaluator for Diagnostic Narratives](https://arxiv.org/abs/2511.09195) \\
**Yuhao Shen#**, Jiahe Qian#, Shuping Zhang#, Zhangtianyi Chen, Tao Lu, Juexiao Zhou*

<!-- [**Benchmark**](https://github.com/placeholder/DermBench) \| [**Project**](https://github.com/placeholder/DermEval)  <strong><span class='show_paper_citations' data='PLACEHOLDER_DERMBENCH'></span></strong> -->

- Propose **DermBench** (six clinical dimensions) and **DermEval** (reference-free evaluator) for imageâ€“text dermatology reasoning aligned with physician scoring.
- Curate dermatologist-verified imageâ€“narrative pairs; standardized reporting for safety, groundedness, and reasoning quality.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2026 Â· Under Review</div><img src='images/ttt.png' alt="CoTBox-TTT" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training](https://arxiv.org/abs/2511.12446) \\
Jiahe Qian#, **Yuhao Shen#**, Zhangtianyi Chen, Juexiao Zhou, Peisong Wang

<!-- [**Project**](https://github.com/placeholder/CoTBox-TTT) \| [**Code**](https://github.com/placeholder/CoTBox-TTT)  <strong><span class='show_paper_citations' data='PLACEHOLDER_COTBOX_TTT'></span></strong> -->

- Evidence-first **test-time training** with all backbones frozen; update a small set of continuous soft prompts guided by **visual chain-of-thought boxes**.
- Enforces answer consistency between full image and localized crops; label-free, plug-and-play, robust to domain shift with low compute.
</div>
</div>